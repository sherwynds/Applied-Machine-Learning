{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 hw6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "\n",
    "# Add your imports below\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_std(*args, **kwargs):\n",
    "    \"\"\"Like cross_validate, except also gives the standard deviation of the score\"\"\"\n",
    "    res = pd.DataFrame(cross_validate(*args, **kwargs))\n",
    "    res_mean = res.mean()\n",
    "\n",
    "    res_mean[\"std_test_score\"] = res[\"test_score\"].std()\n",
    "    if \"train_score\" in res:\n",
    "        res_mean[\"std_train_score\"] = res[\"train_score\"].std()\n",
    "    return res_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "In this assignment we'll look at the [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). The task is to predict whether a text message (SMS) is spam or not spam (\"ham\"). **Sorry for the offensive language in some text messages. If you are sensitive to such language you may wish to avoid reading the raw messages. I have attempted to design the assignment so that any messages you need to read are not disturbing ones.**\n",
    "\n",
    "You should start by downloading the dataset and extracting the csv to your current directory. As usual, please do not commit it to your repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "sms_df = sms_df.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\n",
    "sms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>spam</td>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeah that's what I thought, lemme know if anyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hello, yeah i've just got out of the bath and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>ham</td>\n",
       "      <td>You do what all you like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>ham</td>\n",
       "      <td>Are you planning to come chennai?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target                                                sms\n",
       "647    spam  PRIVATE! Your 2003 Account Statement for shows...\n",
       "3843    ham  Yeah that's what I thought, lemme know if anyt...\n",
       "3044    ham  Hello, yeah i've just got out of the bath and ...\n",
       "2536    ham                           You do what all you like\n",
       "4644    ham                  Are you planning to come chennai?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(sms_df, random_state=123)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the assignment, we'll build a classification model to predict whether a message is spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[\"sms\"]\n",
    "y_train = df_train[\"target\"]\n",
    "\n",
    "X_test = df_test[\"sms\"]\n",
    "y_test = df_test[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "rubric={points:25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `CountVectorizer` to create features from the text data.\n",
    "- Choose an appropriate baseline model (`DummyClassifier` or `DummyRegressor` to predict spam vs. ham and report the relevant scores\n",
    "- Choose an appropriate linear model (`LogisticRegression` or `Ridge`) to predict spam vs. ham\n",
    "- Choose an appropriate random forest model (`RandomForestClassifier` or `RandomForestRegressor`) to predict spam vs. ham\n",
    "- Report the relevant scores for your two models above. You can keep default hyperparameters for simplicity.\n",
    "- Report the most important features according to your linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to display results nicely\n",
    "\n",
    "def cross_validate_metrics(model, X, y, num_folds=5):\n",
    "    scorers = {'Accuracy': make_scorer(accuracy_score), 'Precision': make_scorer(precision_score, pos_label='spam'), 'Recall': make_scorer(recall_score, pos_label='spam'), 'F1': make_scorer(f1_score, pos_label='spam')}\n",
    "    results = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1'], index=['Mean Validation Score', 'Standard Deviation of Validation Score'])\n",
    "    for scorer in scorers.keys():\n",
    "        cv_scores = cross_validate_std(model, X, y, cv=num_folds, scoring=scorers[scorer])\n",
    "        results.loc['Mean Validation Score', scorer] = cv_scores.loc['test_score']\n",
    "        results.loc['Standard Deviation of Validation Score', scorer] = cv_scores.loc['std_test_score']\n",
    "    return results\n",
    "\n",
    "def score_with_metrics(model, X, y):\n",
    "    scorers = {'Accuracy': make_scorer(accuracy_score), 'Precision': make_scorer(precision_score, pos_label='spam'), 'Recall': make_scorer(recall_score, pos_label='spam'), 'F1': make_scorer(f1_score, pos_label='spam')}\n",
    "    results = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1'], index=['Score'])\n",
    "    for scorer in scorers.keys():\n",
    "        results.loc['Score', scorer] = scorers[scorer](model, X, y)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Model - `DummyClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier Cross-validation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Validation</th>\n",
       "      <th>Standard Deviation of Validation</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.862168</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.877243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean Validation  Standard Deviation of Validation      Test\n",
       "Score         0.862168                          0.000521  0.877243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note we do not compute additional metrics here because of 0 division error\n",
    "dc = DummyClassifier(strategy='prior')\n",
    "cv_results = cross_validate_std(dc, X_train, y_train)\n",
    "dc.fit(X_train, y_train)\n",
    "test_score = dc.score(X_test, y_test)\n",
    "dc_results = pd.DataFrame(columns=['Mean Validation', 'Standard Deviation of Validation', 'Test'], index=['Score'])\n",
    "print('Dummy Classifier Cross-validation Scores:')\n",
    "dc_results['Mean Validation'] = cv_results['test_score']\n",
    "dc_results['Standard Deviation of Validation'] = cv_results['std_test_score']\n",
    "dc_results['Test'] = test_score\n",
    "dc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Model - `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Cross-validation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.978943</td>\n",
       "      <td>0.99802</td>\n",
       "      <td>0.848936</td>\n",
       "      <td>0.916639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00749731</td>\n",
       "      <td>0.00442786</td>\n",
       "      <td>0.0552707</td>\n",
       "      <td>0.0329232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy   Precision     Recall  \\\n",
       "Mean Validation Score                     0.978943     0.99802   0.848936   \n",
       "Standard Deviation of Validation Score  0.00749731  0.00442786  0.0552707   \n",
       "\n",
       "                                               F1  \n",
       "Mean Validation Score                    0.916639  \n",
       "Standard Deviation of Validation Score  0.0329232  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(stop_words='english')\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)\n",
    "linear = Pipeline([('countvec', countvec), ('lr', lr)])\n",
    "print('Linear Model Cross-validation Scores:')\n",
    "cross_validate_metrics(linear, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Test Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.984207</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871345</td>\n",
       "      <td>0.93125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall       F1\n",
       "Score  0.984207         1  0.871345  0.93125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.fit(X_train, y_train)\n",
    "print('Linear Model Test Scores:')\n",
    "score_with_metrics(linear, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Model - `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross-validation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.974397</td>\n",
       "      <td>0.989754</td>\n",
       "      <td>0.822834</td>\n",
       "      <td>0.897606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00777636</td>\n",
       "      <td>0.00993107</td>\n",
       "      <td>0.0587536</td>\n",
       "      <td>0.0350101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy   Precision     Recall  \\\n",
       "Mean Validation Score                     0.974397    0.989754   0.822834   \n",
       "Standard Deviation of Validation Score  0.00777636  0.00993107  0.0587536   \n",
       "\n",
       "                                               F1  \n",
       "Mean Validation Score                    0.897606  \n",
       "Standard Deviation of Validation Score  0.0350101  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=123)\n",
    "random_forest = Pipeline([('countvec', countvec), ('rf', rf)])\n",
    "print('Random Forest Cross-validation Scores:')\n",
    "cross_validate_metrics(random_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.972721</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.783626</td>\n",
       "      <td>0.875817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall        F1\n",
       "Score  0.972721  0.992593  0.783626  0.875817"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest.fit(X_train, y_train)\n",
    "print('Random Forest Test Scores:')\n",
    "score_with_metrics(random_forest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importances from Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most Important Features (largest absolute value of coefficients, in order of most to least important):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>2.210630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>2.003192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>1.978341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile</th>\n",
       "      <td>1.951308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txt</th>\n",
       "      <td>1.868370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1.860522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.804746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>1.756085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>won</th>\n",
       "      <td>1.688090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message</th>\n",
       "      <td>1.613170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Coefficient\n",
       "uk          2.210630\n",
       "service     2.003192\n",
       "claim       1.978341\n",
       "mobile      1.951308\n",
       "txt         1.868370\n",
       "new         1.860522\n",
       "50          1.804746\n",
       "150p        1.756085\n",
       "won         1.688090\n",
       "message     1.613170"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(linear.named_steps['lr'].coef_, columns=linear.named_steps['countvec'].get_feature_names()).T.rename(columns={0: 'Coefficient'})\n",
    "feature_importances = feature_importances.reindex(feature_importances.Coefficient.abs().sort_values(ascending=False).index)\n",
    "print('10 Most Important Features (largest absolute value of coefficients, in order of most to least important):')\n",
    "feature_importances.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Let's now try to use pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, using pre-trained word embeddings is very common in NLP. These embeddings are created by training a model like [Word2vec](https://en.wikipedia.org/wiki/Word2vec) on a huge corpus of text. In this exercise we will use a package called [spaCy](https://spacy.io/). Unfortunately, I didn't anticipate using spaCy at the start of the course, and thus it was not included in your course environment. You will need to install it now. Perform the following steps:\n",
    "\n",
    "1. Open a terminal and activate your cpsc330env environment\n",
    "2. Run `conda install spacy` if you're using conda and `cpsc330env.yml`, or `pip install spacy` if you're using pip and `requirements.txt`\n",
    "3. Run `python -m spacy download en_core_web_md`\n",
    "\n",
    "The last line downloads the trained language model itself, called [`en_core_web_md`](https://spacy.io/models/en#en_core_web_md). It is about 50 MB.\n",
    "\n",
    "When you are done, the following line of could should run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are issues, please ask for help on Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(a)\n",
    "rubric={points:5}\n",
    "\n",
    "Our pre-trained `en_core_web_md` model gets us a vector representation of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203694 Identifier Code: 40533 Expires 31/10/04'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.31907508e-02,  1.87776491e-01, -3.50111164e-02, -1.40975965e-02,\n",
       "        6.48319796e-02, -6.47449717e-02,  1.70375593e-02, -6.10386357e-02,\n",
       "       -1.64511222e-02,  1.32269633e+00, -2.01264426e-01,  1.75327167e-01,\n",
       "       -1.92099977e-02, -7.02716932e-02, -4.06776555e-02,  4.39935699e-02,\n",
       "       -9.40892398e-02,  1.16417742e+00,  5.21034896e-02, -4.15729620e-02,\n",
       "       -6.67720437e-02,  2.09751315e-02, -2.88010016e-02, -1.74403172e-02,\n",
       "        7.59456381e-02,  1.59054156e-02, -1.19963408e-01, -4.00383957e-02,\n",
       "        6.05901927e-02,  6.71593547e-02,  9.76373553e-02,  3.17107029e-02,\n",
       "        3.25630940e-02,  1.04522884e-01, -4.10015620e-02, -1.43015515e-02,\n",
       "       -5.64244464e-02,  8.83771945e-03, -5.06912246e-02, -5.84236719e-02,\n",
       "       -7.24398345e-02, -6.05546385e-02,  1.38748854e-01, -1.05185993e-01,\n",
       "       -1.15349621e-01,  8.07616413e-02, -3.67925540e-02,  1.33814156e-01,\n",
       "        4.17499691e-02,  1.61221206e-01, -7.78133571e-02,  2.88000386e-02,\n",
       "       -8.40148050e-03, -6.04870506e-02, -3.08934450e-02, -8.52895677e-02,\n",
       "       -6.39548749e-02,  5.22800349e-03, -5.53854462e-03, -8.03087745e-03,\n",
       "       -1.88329201e-02,  8.88980925e-02, -7.52621666e-02,  6.61760196e-02,\n",
       "        9.47163776e-02,  6.73259944e-02,  1.04649670e-01, -6.46274583e-03,\n",
       "        5.36372103e-02,  1.54217467e-01,  3.08738761e-02,  6.25116751e-02,\n",
       "        3.42663415e-02, -6.89509287e-02,  1.41891614e-01,  9.79844108e-02,\n",
       "        6.95712864e-02, -7.67851481e-03,  7.25936284e-03,  2.37248063e-01,\n",
       "       -1.63150784e-02,  9.07422230e-02, -7.68659562e-02,  7.80237094e-02,\n",
       "        2.84451153e-02, -4.61706370e-02,  2.53731340e-01, -3.38298380e-01,\n",
       "        1.84761882e-01, -3.72366421e-02, -2.91331881e-03,  7.84042478e-02,\n",
       "        6.67143166e-02, -9.97413471e-02,  5.50127588e-02, -4.94359583e-02,\n",
       "       -1.24442577e-01,  4.32233177e-02,  8.62295628e-02,  7.72144422e-02,\n",
       "       -6.07400313e-02,  4.92347144e-02, -4.63975184e-02, -3.30802724e-02,\n",
       "        1.39270853e-02, -8.47563967e-02,  1.25425041e-01,  9.18363184e-02,\n",
       "       -3.94890830e-02,  1.06346551e-02,  6.09008074e-02, -1.80116773e-01,\n",
       "        1.22594684e-01, -1.50459651e-02, -3.37783806e-02,  3.83136868e-02,\n",
       "        8.98202434e-02,  3.74999195e-02, -9.69680026e-03,  9.95773077e-03,\n",
       "        1.75638404e-02, -2.54314896e-02, -8.56809989e-02,  1.08872645e-01,\n",
       "       -1.35684311e-01,  7.97398097e-04,  1.56500377e-02, -8.88168886e-02,\n",
       "        6.03115074e-02, -1.42285809e-01,  1.95733942e-02,  5.01425639e-02,\n",
       "        1.51783610e-02, -1.14804238e-01, -4.64538559e-02,  9.76584703e-02,\n",
       "        3.60706337e-02, -4.66540866e-02, -5.73624186e-02,  8.85936916e-02,\n",
       "       -5.84793568e-01,  5.83318807e-02,  1.49212033e-01,  2.50981990e-02,\n",
       "        1.86520796e-02,  6.34719208e-02,  6.20214045e-02,  1.50626272e-01,\n",
       "       -8.13386738e-02, -3.68060134e-02,  4.65319045e-02,  3.72871198e-02,\n",
       "       -6.42946979e-04, -7.21467882e-02,  9.57818106e-02,  2.75342744e-02,\n",
       "       -5.18624205e-04,  5.84824430e-03, -1.02520809e-02, -2.07720883e-02,\n",
       "        1.33938760e-01, -9.60807800e-02,  5.48885092e-02, -9.11142752e-02,\n",
       "       -1.69003427e-01,  2.58410200e-02,  1.53924776e-02,  7.84069002e-02,\n",
       "        5.00522815e-02,  7.43276328e-02,  1.77948698e-02,  6.87530190e-02,\n",
       "        3.02847251e-02, -2.33742408e-02,  4.10722271e-02,  8.40279385e-02,\n",
       "        3.33111873e-03, -9.05794725e-02, -9.32442099e-02,  9.66621563e-02,\n",
       "        2.35858392e-02, -5.38347214e-02,  5.32682659e-03, -1.45756781e-01,\n",
       "       -1.08654968e-01,  2.74618287e-02, -7.30003044e-02,  1.46668414e-02,\n",
       "        2.07207054e-02, -2.98349615e-02, -9.58048403e-02,  5.70857637e-02,\n",
       "       -9.08484757e-02,  7.41986334e-02,  5.45235164e-02,  2.15741992e-02,\n",
       "       -1.37075856e-01, -1.54765919e-01, -2.30951589e-02,  1.90954208e-01,\n",
       "        7.80405402e-02, -3.37098427e-02, -1.24667056e-01,  1.66881010e-01,\n",
       "        4.03063074e-02,  3.49688977e-02, -6.41835630e-02,  1.62057340e-01,\n",
       "       -4.78211604e-02, -5.11812232e-03,  9.41587612e-02, -3.66693996e-02,\n",
       "       -7.99198225e-02, -2.29402333e-02,  1.28512487e-01,  8.86656567e-02,\n",
       "        1.14268409e-02,  4.05594148e-02, -2.45743275e-01,  1.31086126e-01,\n",
       "        4.34480421e-02,  1.08615510e-01, -7.89643228e-02,  6.92738742e-02,\n",
       "        6.81332219e-03, -7.14639574e-02,  7.93786794e-02,  6.37667254e-02,\n",
       "        3.41754258e-02, -6.88679144e-02, -2.53196377e-02, -7.20241964e-02,\n",
       "       -2.71151252e-02, -5.73015912e-03, -1.21422760e-01,  1.30080767e-02,\n",
       "       -1.32366702e-01,  3.39249009e-03, -1.61491737e-01,  1.08716361e-01,\n",
       "        4.12480384e-02, -3.63959000e-02,  6.74781576e-02,  1.53123200e-01,\n",
       "        1.40259579e-01, -1.95193708e-01, -5.92055777e-03,  6.01037927e-02,\n",
       "       -1.37161344e-01,  1.56843606e-02,  4.05684412e-02,  1.63902566e-01,\n",
       "       -1.11277159e-02, -4.15833183e-02,  1.61682032e-02,  2.99643606e-01,\n",
       "       -4.89742309e-02, -1.15692608e-01,  9.21460614e-03, -4.49603982e-02,\n",
       "        4.27223295e-02,  1.21151373e-01,  1.00167096e-01,  5.83961345e-02,\n",
       "       -6.42061159e-02, -1.97696835e-01, -2.01423150e-02,  4.26647179e-02,\n",
       "        4.05064762e-01, -2.10693106e-01, -2.06212047e-02,  2.43830793e-02,\n",
       "        2.26350017e-02, -8.61471612e-03, -3.57177928e-02, -2.33990122e-02,\n",
       "        1.02524403e-02, -4.58227918e-02,  7.18510821e-02,  7.69190863e-02,\n",
       "        1.21984862e-01,  1.06863193e-02,  9.95338410e-02, -8.44649523e-02,\n",
       "       -5.41926771e-02,  1.26395607e-02,  4.98240627e-03, -3.34332464e-03,\n",
       "        4.26749624e-02,  4.25093882e-02, -8.02726075e-02,  1.31248131e-01,\n",
       "       -7.85700902e-02,  2.75639677e-03,  5.61666302e-02, -7.87066743e-02,\n",
       "       -9.57626700e-02,  1.00734020e-02, -7.58836325e-03,  1.03175854e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(X_train.iloc[0]).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is analogous to calling `transform` with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the _length of the representation_ for these embeddings vs. the `CountVectorizer` approach. Then, compare the _number of nonzero entries_ for the two repesentations of the first training example. Briefly discuss.\n",
    "\n",
    "Note: As briefly discussed in Lecture 14, a common error here is that scikit-learn methods expect certain data shapes as their input. To address this you can use `X_train.iloc[[0]]` instead of `X_train.iloc[0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the first training example:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nonzero Entries</th>\n",
       "      <th>Length of Representation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Spacy</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountVectorizer</th>\n",
       "      <td>16</td>\n",
       "      <td>7151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Nonzero Entries  Length of Representation\n",
       "Spacy                        300                       300\n",
       "CountVectorizer               16                      7151"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_ex_spacy = nlp(X_train.iloc[0]).vector\n",
    "first_ex_countvec = countvec.fit_transform(X_train).toarray()[0]\n",
    "print('From the first training example:')\n",
    "table = pd.DataFrame(data={'Nonzero Entries': [np.count_nonzero(first_ex_spacy), np.count_nonzero(first_ex_countvec)], 'Length of Representation': [len(first_ex_spacy), len(first_ex_countvec)]}, index=['Spacy', 'CountVectorizer'])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "\n",
    "The `spacy` approach leads to a more dense embedding, as there are a high amount of nonzero entries (300/300). However, the `CountVectorizer` approach leads to a more sparse embedding, as there are many more features but generally fewer nonzero entries (16/7151). I'm not sure which will be more effective, but the `CountVectorizer` embedding is certainly more interpretable, because it simply a count of the features (words) for each example, whereas `spacy` is doing something more opaque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b)\n",
    "rubric={points:5}\n",
    "\n",
    "In Exercise 1 you used `CountVectorizer` to generate features, which were then fed into a model. We can do the same here with the features from the pre-trained embedding model. \n",
    "\n",
    "In this case, for computational reasons I will first get the embeddings for the entire train and test sets (note that this doesn't violate the Golden Rule because the transformation is independent for each example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embeddings = pd.DataFrame([sms.vector for sms in nlp.pipe(X_train)])\n",
    "X_test_embeddings  = pd.DataFrame([sms.vector for sms in nlp.pipe(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of scores can you get with these features instead? Compare with your scores from Exercise 1 and briefly discuss. Again, it's fine to stick to default hyperparameters to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "Note: Dummy classifier was not necessary because it doesn't depend on the features, so the results from the embeddings will be the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from Spacy embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.964106</td>\n",
       "      <td>0.900167</td>\n",
       "      <td>0.833238</td>\n",
       "      <td>0.864507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00224385</td>\n",
       "      <td>0.0229232</td>\n",
       "      <td>0.0403669</td>\n",
       "      <td>0.0127527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy  Precision     Recall  \\\n",
       "Mean Validation Score                     0.964106   0.900167   0.833238   \n",
       "Standard Deviation of Validation Score  0.00224385  0.0229232  0.0403669   \n",
       "\n",
       "                                               F1  \n",
       "Mean Validation Score                    0.864507  \n",
       "Standard Deviation of Validation Score  0.0127527  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from Spacy embeddings:')\n",
    "cross_validate_metrics(lr, X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from CountVectorizer embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.978943</td>\n",
       "      <td>0.99802</td>\n",
       "      <td>0.848936</td>\n",
       "      <td>0.916639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00749731</td>\n",
       "      <td>0.00442786</td>\n",
       "      <td>0.0552707</td>\n",
       "      <td>0.0329232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy   Precision     Recall  \\\n",
       "Mean Validation Score                     0.978943     0.99802   0.848936   \n",
       "Standard Deviation of Validation Score  0.00749731  0.00442786  0.0552707   \n",
       "\n",
       "                                               F1  \n",
       "Mean Validation Score                    0.916639  \n",
       "Standard Deviation of Validation Score  0.0329232  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from CountVectorizer embeddings:')\n",
    "cross_validate_metrics(linear, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from Spacy embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.971523</td>\n",
       "      <td>0.98334</td>\n",
       "      <td>0.807181</td>\n",
       "      <td>0.886063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00607234</td>\n",
       "      <td>0.0185804</td>\n",
       "      <td>0.0418557</td>\n",
       "      <td>0.026301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy  Precision     Recall  \\\n",
       "Mean Validation Score                     0.971523    0.98334   0.807181   \n",
       "Standard Deviation of Validation Score  0.00607234  0.0185804  0.0418557   \n",
       "\n",
       "                                              F1  \n",
       "Mean Validation Score                   0.886063  \n",
       "Standard Deviation of Validation Score  0.026301  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from Spacy embeddings:')\n",
    "cross_validate_metrics(rf, X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from CountVectorizer embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Validation Score</th>\n",
       "      <td>0.974397</td>\n",
       "      <td>0.989754</td>\n",
       "      <td>0.822834</td>\n",
       "      <td>0.897606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation of Validation Score</th>\n",
       "      <td>0.00777636</td>\n",
       "      <td>0.00993107</td>\n",
       "      <td>0.0587536</td>\n",
       "      <td>0.0350101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Accuracy   Precision     Recall  \\\n",
       "Mean Validation Score                     0.974397    0.989754   0.822834   \n",
       "Standard Deviation of Validation Score  0.00777636  0.00993107  0.0587536   \n",
       "\n",
       "                                               F1  \n",
       "Mean Validation Score                    0.897606  \n",
       "Standard Deviation of Validation Score  0.0350101  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from CountVectorizer embeddings:')\n",
    "cross_validate_metrics(random_forest, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "For both linear and random forest models, using the `CountVectorizer` embedding achieved mostly similar, but slightly better scores when compared to their `spacy` counterparts. In particular, the linear model had substantially better precision and f1 when using `CountVectorizer`'s embeddings, whereas for the random forest model, the scores were still higher for `CountVectorizer`, but fairly close. We would need to look at the test data to ensure we haven't overfitted the validation set on either model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c)\n",
    "rubric={points:1}\n",
    "\n",
    "Score your models on the test data. Are the results what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from Spacy embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.967696</td>\n",
       "      <td>0.89375</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.864048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall        F1\n",
       "Score  0.967696   0.89375  0.836257  0.864048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from Spacy embeddings:')\n",
    "lr.fit(X_train_embeddings, y_train)\n",
    "score_with_metrics(lr, X_test_embeddings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from CountVectorizer embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.984207</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871345</td>\n",
       "      <td>0.93125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall       F1\n",
       "Score  0.984207         1  0.871345  0.93125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from CountVectorizer embeddings:')\n",
    "linear.fit(X_train, y_train)\n",
    "score_with_metrics(linear, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from Spacy embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.974874</td>\n",
       "      <td>1</td>\n",
       "      <td>0.795322</td>\n",
       "      <td>0.885993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall        F1\n",
       "Score  0.974874         1  0.795322  0.885993"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from Spacy embeddings:')\n",
    "rf.fit(X_train_embeddings, y_train)\n",
    "score_with_metrics(rf, X_test_embeddings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores from CountVectorizer embeddings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>0.972721</td>\n",
       "      <td>0.992593</td>\n",
       "      <td>0.783626</td>\n",
       "      <td>0.875817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy Precision    Recall        F1\n",
       "Score  0.972721  0.992593  0.783626  0.875817"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scores from CountVectorizer embeddings:')\n",
    "random_forest.fit(X_train, y_train)\n",
    "score_with_metrics(random_forest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discussion\n",
    "As expected (given the previous section), the `CountVectorizer` embeddings performed better on the linear model. However, the `spacy` embeddings performed slightly better in all metrics on the random forest model, but not by a significant amount. The test results are similar to the train results, meaning we haven't overfitted validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now we're done with trying to predict class (spam vs. ham). Our next task will be trying to find similar messages to a query message using nearest neighbours, like the product recommendations we discussed in Lecture 14."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a) \n",
    "rubric={points:5}\n",
    "\n",
    "Using scikit-learn's `NearestNeighbours` on the word count features from `CountVectorizer`, searching the training data to find the 5 most similar messages to this made-up message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sms = \"Hey how about some CPSC 330 studying over Zoom or socially distanced at a park? This course is so much fun right?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Euclidean distance and the same `CountVectorizer` you used in Exercise 1.\n",
    "\n",
    "Note: The `kneighbors` function returns indices of the neighbours. To retrieve the corresponding messages, I recommend indexing using the `iloc` syntax.\n",
    "\n",
    "Note: We don't exactly have a notion of train and test anymore, because we're not doing supervised learning anymore. I just picked the training set for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display neighbors nicely\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "def show_neighbors(X, neighbors):\n",
    "    results = pd.DataFrame(columns=neighbors[1][0], index=['Message'])\n",
    "    for column in results.columns:\n",
    "        results.loc['Message', column] = X.iloc[column]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec.fit(X_train)\n",
    "X_train_countvec = pd.DataFrame(data=countvec.transform(X_train).toarray(), columns=countvec.get_feature_names(), index=X_train.index)\n",
    "query_sms_countvec = countvec.transform([query_sms])\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_train_countvec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest neighbors, in order of similarity (left to right):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>154</th>\n",
       "      <th>1685</th>\n",
       "      <th>1684</th>\n",
       "      <th>58</th>\n",
       "      <th>1924</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Message</th>\n",
       "      <td>:)</td>\n",
       "      <td>You can never do NOTHING</td>\n",
       "      <td>Can a not?</td>\n",
       "      <td>Hey u still at the gym?</td>\n",
       "      <td>What about this one then.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        154                       1685        1684                     58    \\\n",
       "Message  :)   You can never do NOTHING  Can a not?  Hey u still at the gym?   \n",
       "\n",
       "                              1924  \n",
       "Message  What about this one then.  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = nn.kneighbors(query_sms_countvec.toarray())\n",
    "print(\"Closest neighbors, in order of similarity (left to right):\")\n",
    "show_neighbors(X_train, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={points:2}\n",
    "\n",
    "Repeat part (a) but using cosine similarity instead of Euclidean distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "nn.fit(X_train_countvec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest neighbors, in order of similarity (left to right):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>206</th>\n",
       "      <th>264</th>\n",
       "      <th>3326</th>\n",
       "      <th>58</th>\n",
       "      <th>1462</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Message</th>\n",
       "      <td>Your right! I'll make the appointment right now.</td>\n",
       "      <td>Of course. I guess god's just got me on hold right now.</td>\n",
       "      <td>\\HEY KATE</td>\n",
       "      <td>Hey u still at the gym?</td>\n",
       "      <td>Lmao but its so fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     206   \\\n",
       "Message  Your right! I'll make the appointment right now.   \n",
       "\n",
       "                                                            264        3326  \\\n",
       "Message  Of course. I guess god's just got me on hold right now.  \\HEY KATE   \n",
       "\n",
       "                            58                      1462  \n",
       "Message  Hey u still at the gym?  Lmao but its so fun...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = nn.kneighbors(query_sms_countvec.toarray())\n",
    "print(\"Closest neighbors, in order of similarity (left to right):\")\n",
    "show_neighbors(X_train, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={points:5}\n",
    "\n",
    "In lecture we talked about how Euclidean distance resulted in less popular items being recommended than with cosine similarity. What is the analog of \"popularity\" here? Are your results from parts (a) and (b) consistent with this notion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "Here, the analog of popularity is the number of words in the string. In the same way that popular products are more likely to be similar in some way to a query item, strings wtih more words are more likely to contain some similar meaning to a query string. The results from (a) and (b) are somewhat consistent with this notion, as cosine similarity (b) found longer messages than euclidean distance (a) on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(d)\n",
    "rubric={points:3}\n",
    "\n",
    "Repeat parts (a) and (b) but this time with the pre-trained embeddings from Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sms_spacy = nlp(query_sms).vector\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_train_embeddings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest neighbors, in order of similarity (left to right):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1642</th>\n",
       "      <th>754</th>\n",
       "      <th>4107</th>\n",
       "      <th>1602</th>\n",
       "      <th>861</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Message</th>\n",
       "      <td>Have you heard about that job? I'm going to that wildlife talk again tonight if u want2come. Its that2worzels and a wizzle or whatever it is?!</td>\n",
       "      <td>Not a lot has happened here. Feels very quiet. Beth is at her aunts and charlie is working lots. Just me and helen in at the mo. How have you been?</td>\n",
       "      <td>Do whatever you want. You know what the rules are. We had a talk earlier this week about what had to start happening, you showing responsibility. Yet, every week it's can i bend the rule this way? What about that way? Do whatever. I'm tired of having thia same argument with you every week. And a  &amp;lt;#&amp;gt;  movie DOESNT inlude the previews. You're still getting in after 1.</td>\n",
       "      <td>Exactly. Anyways how far. Is jide her to study or just visiting</td>\n",
       "      <td>Oh ! A half hour is much longer in Syria than Canada, eh ? Wow you must get SO much more work done in a day than us with all that extra time ! *grins*</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    1642  \\\n",
       "Message  Have you heard about that job? I'm going to that wildlife talk again tonight if u want2come. Its that2worzels and a wizzle or whatever it is?!    \n",
       "\n",
       "                                                                                                                                                         754   \\\n",
       "Message  Not a lot has happened here. Feels very quiet. Beth is at her aunts and charlie is working lots. Just me and helen in at the mo. How have you been?    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                            4107  \\\n",
       "Message  Do whatever you want. You know what the rules are. We had a talk earlier this week about what had to start happening, you showing responsibility. Yet, every week it's can i bend the rule this way? What about that way? Do whatever. I'm tired of having thia same argument with you every week. And a  &lt;#&gt;  movie DOESNT inlude the previews. You're still getting in after 1.   \n",
       "\n",
       "                                                                    1602  \\\n",
       "Message  Exactly. Anyways how far. Is jide her to study or just visiting   \n",
       "\n",
       "                                                                                                                                                           861   \n",
       "Message  Oh ! A half hour is much longer in Syria than Canada, eh ? Wow you must get SO much more work done in a day than us with all that extra time ! *grins*  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = nn.kneighbors([query_sms_spacy])\n",
    "print(\"Closest neighbors, in order of similarity (left to right):\")\n",
    "show_neighbors(X_train, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=5, metric='cosine')\n",
    "nn.fit(X_train_embeddings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest neighbors, in order of similarity (left to right):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1642</th>\n",
       "      <th>754</th>\n",
       "      <th>1602</th>\n",
       "      <th>861</th>\n",
       "      <th>4107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Message</th>\n",
       "      <td>Have you heard about that job? I'm going to that wildlife talk again tonight if u want2come. Its that2worzels and a wizzle or whatever it is?!</td>\n",
       "      <td>Not a lot has happened here. Feels very quiet. Beth is at her aunts and charlie is working lots. Just me and helen in at the mo. How have you been?</td>\n",
       "      <td>Exactly. Anyways how far. Is jide her to study or just visiting</td>\n",
       "      <td>Oh ! A half hour is much longer in Syria than Canada, eh ? Wow you must get SO much more work done in a day than us with all that extra time ! *grins*</td>\n",
       "      <td>Do whatever you want. You know what the rules are. We had a talk earlier this week about what had to start happening, you showing responsibility. Yet, every week it's can i bend the rule this way? What about that way? Do whatever. I'm tired of having thia same argument with you every week. And a  &amp;lt;#&amp;gt;  movie DOESNT inlude the previews. You're still getting in after 1.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    1642  \\\n",
       "Message  Have you heard about that job? I'm going to that wildlife talk again tonight if u want2come. Its that2worzels and a wizzle or whatever it is?!    \n",
       "\n",
       "                                                                                                                                                         754   \\\n",
       "Message  Not a lot has happened here. Feels very quiet. Beth is at her aunts and charlie is working lots. Just me and helen in at the mo. How have you been?    \n",
       "\n",
       "                                                                    1602  \\\n",
       "Message  Exactly. Anyways how far. Is jide her to study or just visiting   \n",
       "\n",
       "                                                                                                                                                           861   \\\n",
       "Message  Oh ! A half hour is much longer in Syria than Canada, eh ? Wow you must get SO much more work done in a day than us with all that extra time ! *grins*   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                            4107  \n",
       "Message  Do whatever you want. You know what the rules are. We had a talk earlier this week about what had to start happening, you showing responsibility. Yet, every week it's can i bend the rule this way? What about that way? Do whatever. I'm tired of having thia same argument with you every week. And a  &lt;#&gt;  movie DOESNT inlude the previews. You're still getting in after 1.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = nn.kneighbors([query_sms_spacy])\n",
    "print(\"Closest neighbors, in order of similarity (left to right):\")\n",
    "show_neighbors(X_train, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(e)\n",
    "rubric={points:2}\n",
    "\n",
    "Our first approach, using `CountVectorizer` features, should only retrieve similar messages if they have some words in common with the query message. Is this also true for the pre-trained embedding approach as well? Briefly discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "This is not necessarily true of the pre-trained embeddings, because the features in each embedding vector are not counts of words, but are rather some opaque embeddings created by the `en_core_web_md` model, that aren't easy to interpret. While the result is some of the similar messages found share words with the query, these embeddings might be capturing more general sentiment beyond simply the words present in the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(f)\n",
    "rubric={points:2}\n",
    "\n",
    "In class we talked about how, when using pre-trained models, it's important that the original training data was somewhat similar to our own data. For example, in Lecture 13 we talked about how the dog breed images were fairly similar to ImageNet images. We're using the `en_core_web_md` pre-trained model from spaCy - its documentation is [here](https://spacy.io/models/en#en_core_web_md). Based on the documentation, it seems like the word vectors come from [Common Crawl](https://commoncrawl.org/). Do you think that training data is suitable for turning these SMS messages into feature vectors? Briefly discuss. (There is no single correct answer here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "I think it is fairly similar, and therefore useful, because the web contains huge corpus of language crossing different levels of formality, topics, and linguistic complexity, and this could generalize well to a text message that could use a variety of tones, topics, or sentiments. However, SMS data might tend to be shorter, more abbreviated, and more casual/profane, than perhaps the average sample of text on the web, which might make `spacy`'s embeddings less useful. Ultimately, the trade-off is better trained embeddings that have access to a much larger, but slightly different corpus, versus feature embeddings that might be fit to less data but are more similar to the scope of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very short answer questions\n",
    "rubric={points:8}\n",
    "\n",
    "Each question is worth 2 points.\n",
    "\n",
    "The first two questions pertain to the material we skipped in Lecture 12. A screencast is available on the course README; [here](https://www.dropbox.com/s/da7lx8kdzxfmna2/lecture12.mp4?dl=0) is the link for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider using `CountVectorizer(max_features=1000)` and then reducing to 500 features with `RFE(n_features_to_select=500)` vs. using `CountVectorizer(max_features=500)`. Are these two approaches the same? If not, how are they different?\n",
    "2. After running feature selection with `RFE`, `rfe.ranking_` tells you the order in which the features were removed. Why could this order be different from the order of the original feature importances, ranked from least to most important?\n",
    "3. In Lecture 13 we discussed how neural networks are sort of like `Pipeline`s, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "4. In Lecture 15 we saw our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "1. These approaches are different. From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), `max_features=500` will simply select the 500 most frequent features (which might not be the most important) whereas using `RFE` will start with twice the amount of features, and recursively select the most important ones, recomputing the importances during each iteration.\n",
    "2. As per lecture 12, \"A feature's relevance can only be defined in the context of other features; adding/removing features can make features relevant/irrelevant\". Certain features might only be useful in combination with others, so if two features that are only useful together have some importance, eliminating one might make the other (mostly) useless.\n",
    "3. It's useful because transfer learning involves taking pre-trained transformation steps and putting our own model (classifier/regressor) on top of it. Therefore, we can use the pre-trained transformation steps of a neural network that has seen way more data than our own computers can handle, and apply it to a similar data before putting our own model on top of it.\n",
    "4. For example, using a model like that as a job recommendation system for the unemployed might push women towards stereotypically feminine roles like nursing whereas it might push men towards male-dominated fields like programming, reinforcing streotypical gender norms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Canvas\n",
    "\n",
    "**IF YOU ARE WORKING WITH A PARTNER** please form the group before submitting - see instructions [here](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md#partners).\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`.\n",
    "2. Save your notebook.\n",
    "3. Convert your notebook to `.html` format using the `convert_notebook()` function below **or** by `File -> Export Notebook As... -> Export Notebook to HTML`.\n",
    "4. Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    ">For this step, you will need a Canvas *Access Token* token. If you haven't already got one, log-in to Canvas, click `Account` (top-left of the screen), then `Settings`, then scroll down until you see the `+ New Access Token` button. Click that button, give your token any name you like and set the expiry date to Dec 31, 2020. Then click `Generate token`. Save this token in a safe place on your computer as you'll need it for all assignments. Treat the token with as much care as you would an important password. \n",
    "\n",
    "Note: for those having trouble with the Jupyter widgets and the dropdowns: if you add the argument `no_widgets=True` to your `submit` call, it should let you do a text-based entry of your key and avoid the dropdowns altogether. If this doesn't work, you probably need to upgrade to the latest version of `canvasutils` with `pip install canvasutils -U` from your terminal with your environment activated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasutils.submit import submit, convert_notebook\n",
    "\n",
    "# Note: the canvasutils package should have been installed as part of your environment setup - \n",
    "# see https://github.com/UBC-CS/cpsc330/blob/master/docs/setup.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook successfully converted! \n"
     ]
    }
   ],
   "source": [
    "# convert_notebook(\"hw6.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook to HTML (or you can convert manually from the File menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste your token here and then hit enter:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ······································································\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m======================================\u001b[0m\n",
      "Token successfully entered - thanks!\n",
      "\u001b[92m======================================\u001b[0m\n",
      "\n",
      "\n",
      "Select an assignment to submit to:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabbc35594734962a7ef1ff6cc25ffe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# submit(course_code=53561, token=False)  # uncomment and run when ready to submit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
