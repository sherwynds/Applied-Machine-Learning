{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: implementing `DummyClassifier`\n",
    "rubric={points:20}\n",
    "\n",
    "In this course (unlike CPSC 340) you will generally **not** be asked to implement machine learning algorihtms (like logistic regression) from scratch. However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    "\n",
    "As a reminder, `DummyClassifier` is meant as a baseline and is generally the worst possible \"model\" you could \"fit\" to a dataset. All it does is predict the most popular class in the training set. So if there are more 0s than 1s it predicts 0 every time, and if there are more 1s than 0s it predicts 1 every time. For `predict_proba` it looks at the frequencies in the training set, so if you have 30% 0's 70% 1's it predicts `[0.3 0.7]` every time. Thus, `fit` only looks at `y` (not `X`).\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, `predict_proba()` and `score()`. Your task is to fill in those four functions. To get your started, I have given you a `return` statement in each case that returns the correct data type: `fit` can return nothing, `predict` returns an array whose size is the number of examples, `predict_proba` returns an array whose size is the number of examples x 2, and `score` returns a number.\n",
    "\n",
    "The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the frequency of each class in the training data, which is the behaviour of `DummyClassifier(strategy='prior')`. Your `score` function should call your `predict` function. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "To simplify this question, you can assume **binary classification**, and furthermore that these classes are **encoded as 0 and 1**. In other words, you can assume that `y` contains only 0s and 1s. The real `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"mouse\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "    \n",
    "    This implementation only works when y only contains 0s and 1s.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # adapted from https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray-in-python\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        vals = dict(zip(unique, counts))\n",
    "        self.neg = vals[0]/len(y)\n",
    "        self.pos = vals[1]/len(y)\n",
    "        if self.neg < self.pos:\n",
    "            self.dummy = 1\n",
    "        else:\n",
    "            self.dummy = 0               \n",
    "        return None\n",
    "    \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # adapted from https://stackoverflow.com/questions/5891410/numpy-array-initialization-fill-with-identical-values\n",
    "        return np.full(len(X),self.dummy) \n",
    "\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pos = np.full(len(X),self.pos)\n",
    "        neg = np.full(len(X),self.neg)\n",
    "        proba = np.column_stack((neg,pos))\n",
    "        return proba\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        compare = np.column_stack((y,y_pred))\n",
    "        mask = ((compare[:,0]) == (compare[:,1]))\n",
    "        return len(compare[mask])/len(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests for `predict` using randomly generated data. You may want to run the cell a few times to make sure you explore the different cases (or automate this with a loop or random seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, generate random data\n",
    "n_train = 101\n",
    "n_valid = 21\n",
    "d = 5\n",
    "X_train_dummy = np.random.randn(n_train, d)\n",
    "X_valid_dummy = np.random.randn(n_valid, d)\n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "\n",
    "my_dc = MyDummyClassifier()\n",
    "sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "my_dc.fit(X_train_dummy, y_train_dummy);\n",
    "sk_dc.fit(X_train_dummy, y_train_dummy);\n",
    "      \n",
    "assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests for `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy))\n",
    "assert np.allclose(my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests for `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy))\n",
    "assert np.isclose(my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e3cc53df86a7e14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 2: Trump Tweets\n",
    "\n",
    "For the rest of this assignment we'll be looking at a [dataset of Donald Trump's tweets](https://www.kaggle.com/austinreese/trump-tweets) as of June 2020. You should start by downloading the dataset. Unzip it and move the file `realdonaldtrump.csv` into this directory. As usual, please do not commit it to your repos. This assignment hopefully shipped with a `.gitignore` file that will prevent you from committing the dataset to your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         link  \\\n",
       "id                                                              \n",
       "1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                                      content  \\\n",
       "id                                                              \n",
       "1698308935  Be sure to tune in and watch Donald Trump on L...   \n",
       "1701461182  Donald Trump will be appearing on The View tom...   \n",
       "1737479987  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "1741160716  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "1773561338  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                           date  retweets  favorites mentions hashtags  \n",
       "id                                                                      \n",
       "1698308935  2009-05-04 13:54:25       510        917      NaN      NaN  \n",
       "1701461182  2009-05-04 20:00:10        34        267      NaN      NaN  \n",
       "1737479987  2009-05-08 08:38:08        13         19      NaN      NaN  \n",
       "1741160716  2009-05-08 15:40:15        11         26      NaN      NaN  \n",
       "1773561338  2009-05-12 09:07:28      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('realdonaldtrump.csv', index_col=0)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43352, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict whether a tweet will go \"viral\", defined as having more than 10,000 retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_df[\"retweets\"] > 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we'll be using only the content (text) of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this assignment, you can ignore all the other columns in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(a) ordering the steps\n",
    "rubric={points:6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a model using `CountVectorizer` and `LogisticRegression`. The code required to do this has been provided below, but in the wrong order. \n",
    "\n",
    "- Rearrange the lines of code to correctly fit the model and compute the cross-validation score. \n",
    "- Add a short comment to each block to describe what the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       1.730353\n",
       "score_time     0.153433\n",
       "test_score     0.897890\n",
       "train_score    0.967045\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoids violating the golden rule by splitting the features (X) and target (y) into train/test sets (X_train, X_test, y_train, y_test), using a fixed random shuffling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=321)\n",
    "\n",
    "# Creates an unfit CountVectorizer object with a predifined list of 'stop_words' (common english words that will be ignored by CountVectorizer when deciding the vocabulary because they lack useful info)\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Creates a unfit logististicRegression object that can be used to fit/predict/score on data with a max of 1000 iterations for the model to converge at optimal weights\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Creates a data pipeline with the CountVectorizer and LogisticRegression objects created earlier; which, ONLY WHEN fit, will first fit/transform values with 'countvec', then fit the 'logreg' model\n",
    "pipe = Pipeline([('countvec', countvec), ('logreg', lr)])\n",
    "\n",
    "# Computes 5-fold validation scores except following both steps of the pipeline for each fold, and presents the results including the training score\n",
    "cross_val_results = pd.DataFrame(cross_validate(pipe, X_train, y_train, return_train_score=True))\n",
    "\n",
    "# Returns the mean validation score across all 5 folds of cross-validation\n",
    "cross_val_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b) Cross-validation fold sub-scores\n",
    "rubric={points:3}\n",
    "\n",
    "Above we averaged the scores from the 5 folds of cross-validation. \n",
    "\n",
    "- Print out the 5 individual scores. Reminder: sklearn calls them `\"test_score\"` but they are really (cross-)validation scores. \n",
    "- Are the 5 scores close to each other or spread far apart? (This is a bit subjective, answer to the best of your ability.)\n",
    "- How does the size of this dataset (number of rows) compare to the cilantro dataset? How does this relate to the different sub-scores from the 5 folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.899123\n",
      "1    0.899739\n",
      "2    0.896356\n",
      "3    0.898201\n",
      "4    0.896032\n",
      "Name: test_score, dtype: float64\n",
      "Standard deviation of test scores: 0.0014725591589632206\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_results['test_score'])\n",
    "print(f\"Standard deviation of test scores: {np.std(cross_val_results['test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Given the standard deviation above, I believe the 5 validation scores are fairly close, becuase they are on average less than a percentage point away from the mean. This dataset has 43352 total rows compared to the cilantro dataset's 200 (the training data is roughly 75% of this). Because this dataset has significanly more rows, its sub-score validation accuracy is likely to vary less (closer together), as big variances in the distribution become less pronounced. It is like the law of large numbers: as sample size grows, the mean gets closer to the average of the whole population [1]. As the number of data points in each fold increases, the sub-score specific to that fold gets closer to the average validation score.\n",
    "\n",
    "[1] https://www.investopedia.com/terms/l/lawoflargenumbers.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c) baseline\n",
    "rubric={points:3}\n",
    "\n",
    "By the way, are these scores any good? \n",
    "\n",
    "- Run `DummyClassifier` (or `MyDummyClassifier`!) on this dataset.\n",
    "- Compare the `DummyClassifier` score to what you got from logistic regression above. Does logistic regression seem to be doing anything useful?\n",
    "- Is it necessary to use `CountVectorizer` here? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit_time  score_time  test_score  train_score\n",
      "0  0.000000         0.0    0.738582     0.738534\n",
      "1  0.015607         0.0    0.738582     0.738534\n",
      "2  0.000000         0.0    0.738582     0.738534\n",
      "3  0.000000         0.0    0.738428     0.738572\n",
      "4  0.000000         0.0    0.738542     0.738544\n",
      "Average validation score: 0.7385433966473036\n"
     ]
    }
   ],
   "source": [
    "dc = DummyClassifier(strategy='prior')\n",
    "dc.fit(X_train, y_train)\n",
    "cv_scores = pd.DataFrame(cross_validate(dc, X_train, y_train, cv=5, return_train_score=True))\n",
    "print(cv_scores)\n",
    "print(f\"Average validation score: {np.mean(cv_scores['test_score'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Logistic Regression achieves a 5-fold validation score of around 0.90 while Dummy Classifier achieves a 5-fold validation score of around 0.74. Therefore, it seems logistic regression is useful because it performs significanlty better on data it has not seen before (the validation data in each fold). Using `CountVectorizer` is unnecessary, because it is only used to obtain our feature columns, but `DummyClassifier` only looks at the target column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f8ea22638cf75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2(d) probability scores\n",
    "rubric={points:3}\n",
    "\n",
    "Here we train a logistic regression classifier on the entire training set: \n",
    "\n",
    "(Note: this is relying on the `pipe` variable from 2(a) - you'll need to redefine it if you overwrote that variable in between.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, find the tweet in the **test set** with the highest predicted probability of being viral. Print out the tweet and the associated probability score.\n",
    "\n",
    "Reminder: you are free to reuse/adapt code from lecture. Please add in a small attribution, e.g. \"From Lecture 4\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of tweet in the test set with highest predicted probability of being viral: 5134\n",
      "\n",
      "Tweet in the test set with the highest predicted probability of being viral:\n",
      "Corrupt politician Adam Schiff wants people from the White House to testify in his and Pelosi’s disgraceful Witch Hunt, yet he will not allow a White House lawyer, nor will he allow ANY of our requested witnesses. This is a first in due process and Congressional history!\n",
      "\n",
      "Associated probability score: 0.99999993252877\n"
     ]
    }
   ],
   "source": [
    "# From Lecture 4\n",
    "proba = pipe.predict_proba(X_test)\n",
    "viral_tweet_idx = np.argmax(proba[:,1]);\n",
    "print(f\"Index of tweet in the test set with highest predicted probability of being viral: {viral_tweet_idx}\\n\")\n",
    "print(\"Tweet in the test set with the highest predicted probability of being viral:\")\n",
    "print(str(X_test.iloc[viral_tweet_idx]))\n",
    "print(f\"\\nAssociated probability score: {proba[viral_tweet_idx][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f910e9d1d6d09182",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2(e) coefficients\n",
    "rubric={points:3}\n",
    "\n",
    "We can extract the `CountVectorizer` and `LogisticRegression` objects from the `Pipeline` object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_from_pipe = pipe.named_steps['countvec']\n",
    "lr_from_pipe  = pipe.named_steps['logreg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these extracted components above, display the 5 words with the highest coefficients and the 5 words with the smallest coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 words with the highest coefficients (weights):\n",
      "               Weight\n",
      "harassment   2.731773\n",
      "mini         2.712408\n",
      "fake         2.692772\n",
      "coronavirus  2.434191\n",
      "transcripts  2.380542\n",
      "\n",
      "5 words with the lowest coefficients (weights):\n",
      "                   Weight\n",
      "realdonaldtrump -3.116883\n",
      "trump2016pic    -2.637304\n",
      "barackobama     -2.565433\n",
      "trump2016       -2.316150\n",
      "1pic            -2.295131\n"
     ]
    }
   ],
   "source": [
    "# From Lecture 4\n",
    "vocab = vec_from_pipe.get_feature_names()\n",
    "weights = lr_from_pipe.coef_.ravel()\n",
    "words_weights_df = pd.DataFrame(data=weights, index=vocab, columns=['Weight'])\n",
    "words_weights_df = words_weights_df.sort_values(by=\"Weight\", ascending=False)\n",
    "print(\"5 words with the highest coefficients (weights):\")\n",
    "print(words_weights_df[0:5])\n",
    "print(\"\\n5 words with the lowest coefficients (weights):\")\n",
    "print(words_weights_df.iloc[[-1,-2,-3,-4,-5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(f)\n",
    "rubric={points:10}\n",
    "\n",
    "scikit-learn provides a lot of useful tools like `Pipeline`s and `cross_validate`, which are awesome. But with these fancy tools it's also easy to lose track of what is actually happening under the hood. Here, your task is to \"manually\" (without `Pipeline` and without `cross_validate` or `cross_val_score`) compute logistic regression's validation score on one fold (that is, train on 80% and validate on 20%) of the training data. \n",
    "\n",
    "You should start with the following `CountVectorizer` and `LogisticRegression` objects, as well as `X_train` and `y_train` (which you should further split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words='english')\n",
    "lr = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta-comment: you might be wondering why we're going into \"implementation\" here if this course is about _applied_ ML. In CPSC 340, we would go all the way down into `LogisticRegression` and understand how `fit` works, line by line. Here we're not going into that at all, but I still think this type of question (and Exercise 1) is a useful middle ground. I do want you to know what is going on in `Pipeline` and in `cross_validate` even if we don't cover the details of `fit`. To get into logistic regression's `fit` requires a bunch of math; here, we're keeping it more conceptual and avoiding all those prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression validation score on one fold: 0.8966630785791173\n"
     ]
    }
   ],
   "source": [
    "train_amt = int(len(X_train)*0.8)\n",
    "\n",
    "X_train_fold_raw = X_train[:train_amt]\n",
    "X_valid_fold_raw = X_train[train_amt:]\n",
    "y_train_fold = y_train[:train_amt]\n",
    "y_valid_fold = y_train[train_amt:]\n",
    "\n",
    "X_train_fold = countvec.fit_transform(X_train_fold_raw)\n",
    "X_valid_fold = countvec.transform(X_valid_fold_raw)\n",
    "\n",
    "lr.fit(X_train_fold, y_train_fold)\n",
    "score = lr.score(X_valid_fold, y_valid_fold)\n",
    "\n",
    "print(f\"Logistic Regression validation score on one fold: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 3: hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e9e6fdea209d872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 3(a)\n",
    "rubric={points:2}\n",
    "\n",
    "The following code varies the `max_features` hyperparameter of `CountVectorizer` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `max_features`. It also prints the results. Based on the plot/output, what value of `max_features` seems best? Briefly explain.\n",
    "\n",
    "Note: the code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "max_features = [10,100,1000,10_000,100_000]\n",
    "\n",
    "for mf in max_features:\n",
    "#    print(mf)\n",
    "    pipe = Pipeline([('countvec', CountVectorizer(stop_words='english', max_features=mf)), \n",
    "                         ('logreg', LogisticRegression(max_iter=1000))])\n",
    "    cv_results = cross_validate(pipe, X_train, y_train,  return_train_score=True)\n",
    "\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(max_features, train_scores, label=\"train\")\n",
    "plt.semilogx(max_features, cv_scores, label=\"valid\")\n",
    "plt.legend();\n",
    "plt.xlabel(\"max_features\");\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_features</th>\n",
       "      <th>train</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.767854</td>\n",
       "      <td>0.766593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.838900</td>\n",
       "      <td>0.837147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.904633</td>\n",
       "      <td>0.887956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.951498</td>\n",
       "      <td>0.896537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.967045</td>\n",
       "      <td>0.897890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_features     train        cv\n",
       "0            10  0.767854  0.766593\n",
       "1           100  0.838900  0.837147\n",
       "2          1000  0.904633  0.887956\n",
       "3         10000  0.951498  0.896537\n",
       "4        100000  0.967045  0.897890"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"max_features\" : max_features, \"train\" : train_scores, \"cv\" : cv_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of max_features is 100000 because it maximizes the cross-validation score at 0.8978900824847041\n"
     ]
    }
   ],
   "source": [
    "best_max_features = max_features[np.argmax(cv_scores)]\n",
    "print(f\"The best value of max_features is {best_max_features} because it maximizes the cross-validation score at {np.max(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={points:2}\n",
    "\n",
    "The following code varies the `C` hyperparameter of `LogisticRegression` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `C`. Based on the plot, what value of `C` seems best?\n",
    "\n",
    "Note: the code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "C_vals = 10.0**np.arange(-1.5,2,0.5)\n",
    "\n",
    "for C in C_vals:\n",
    "#    print(C)\n",
    "    pipe = Pipeline([('countvec', CountVectorizer(stop_words='english', max_features=None)), \n",
    "                         ('logreg', LogisticRegression(max_iter=1000, C=C))])\n",
    "    cv_results = cross_validate(pipe, X_train, y_train,  return_train_score=True)\n",
    "\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "    \n",
    "plt.semilogx(C_vals, train_scores, label=\"train\")\n",
    "plt.semilogx(C_vals, cv_scores, label=\"valid\")\n",
    "plt.legend();\n",
    "plt.xlabel(\"C\");\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>train</th>\n",
       "      <th>cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.896898</td>\n",
       "      <td>0.878821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.919196</td>\n",
       "      <td>0.893277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.941333</td>\n",
       "      <td>0.897521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967045</td>\n",
       "      <td>0.897890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.985675</td>\n",
       "      <td>0.894507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.993733</td>\n",
       "      <td>0.888171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31.622777</td>\n",
       "      <td>0.996571</td>\n",
       "      <td>0.881282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           C     train        cv\n",
       "0   0.031623  0.896898  0.878821\n",
       "1   0.100000  0.919196  0.893277\n",
       "2   0.316228  0.941333  0.897521\n",
       "3   1.000000  0.967045  0.897890\n",
       "4   3.162278  0.985675  0.894507\n",
       "5  10.000000  0.993733  0.888171\n",
       "6  31.622777  0.996571  0.881282"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"C\" : C_vals, \"train\" : train_scores, \"cv\" : cv_scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value of C is 1.0 because it maximizes the cross-validation score at 0.8978900824847041\n"
     ]
    }
   ],
   "source": [
    "best_C = C_vals[np.argmax(cv_scores)]\n",
    "print(f\"The best value of C is {best_C} because it maximizes the cross-validation score at {np.max(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={points:10}\n",
    "\n",
    "- Using `GridSearchCV`, jointly optimize `max_features` and `C` across all the combinations of values we tried above. \n",
    "  - Note: the code might be a bit slow here. \n",
    "  - Setting `n_jobs=-1` should speed it up if you have a multi-core processor.\n",
    "  - You can reduce the number of folds (e.g. `cv=2`) to speed it up if necessary.\n",
    "- What are the best values of `max_features` and `C` according to your grid search?\n",
    "- Do these best values agree with what you found in parts (a) and (b)?\n",
    "- Generally speaking, _should_ these values agree with what you found in parts (a) and (b)? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   28.4s\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:   43.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Grid Search:\n",
      "The best value of max_features is 100000\n",
      "The best value of C is 1.0\n",
      "\n",
      "The best max_features found agrees with part (a): 100000\n",
      "The best C found agrees with part (a): 1.0\n"
     ]
    }
   ],
   "source": [
    "# From Lecture 5\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('countvec', countvec),\n",
    "    ('lr', lr)])\n",
    "\n",
    "param_grid = {\n",
    "    \"countvec__max_features\": [10,100,1000,10_000,100_000],\n",
    "    \"lr__C\": 10.0**np.arange(-1.5,2,0.5)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train);\n",
    "best = grid_search.best_params_\n",
    "print(f\"According to Grid Search:\\nThe best value of max_features is {best['countvec__max_features']}\\nThe best value of C is {best['lr__C']}\")\n",
    "\n",
    "if best['countvec__max_features'] == best_max_features:\n",
    "    print(f\"\\nThe best max_features found agrees with part (a): {best_max_features}\")\n",
    "else:\n",
    "    print(f\"\\nThe best max_features found: {best['countvec__max_features']} differs from part (a): {best_max_features}\")\n",
    "\n",
    "if best['lr__C'] == best_C:\n",
    "    print(f\"The best C found agrees with part (a): {best_C}\")\n",
    "else:\n",
    "    print(f\"The best C found: {best['lr__C']} differs from part (a): {best_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It makes sense for these values to agree with each other, but in general, it is possible for the values to disagree as well. If there is some relation between the hyperparamaters, optimizing them together (each possible combination) could yeild a different result than optimizing one while holding the other constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(d)\n",
    "rubric={points:3}\n",
    "\n",
    "- Evaluate your final model on the test set. \n",
    "- How does your test accuracy compare to your validation accuracy? \n",
    "- If they are different: do you think this is because you \"overfitted on the validation set\", or simply random luck?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation scores from Grid Search:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_countvec__max_features</th>\n",
       "      <th>param_lr__C</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897890</td>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.897521</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.897029</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.316228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.896537</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.894507</td>\n",
       "      <td>100000</td>\n",
       "      <td>3.16228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score param_countvec__max_features param_lr__C\n",
       "rank_test_score                                                          \n",
       "1                       0.897890                       100000           1\n",
       "2                       0.897521                       100000    0.316228\n",
       "3                       0.897029                        10000    0.316228\n",
       "4                       0.896537                        10000           1\n",
       "5                       0.894507                       100000     3.16228"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From Lecture 5\n",
    "print(\"Validation scores from Grid Search:\")\n",
    "cv_scores = pd.DataFrame(grid_search.cv_results_)[['mean_test_score', 'param_countvec__max_features', 'param_lr__C', 'rank_test_score']].set_index(\"rank_test_score\").sort_index()\n",
    "cv_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test score was: 0.8992434028418528, while the validation score was: 0.8978900824847041\n"
     ]
    }
   ],
   "source": [
    "test_score = grid_search.score(X_test, y_test)\n",
    "validation_score = cv_scores.loc[1,'mean_test_score']\n",
    "print(f\"The test score was: {test_score}, while the validation score was: {validation_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The test and validation accuracy are fairly similar. In fact, the validation accuracy is actually lower than the test accuracy. I think we can attribute this to the random shuffling of train/test values. The closeness between test and validation accuracy increases my trust in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very short answer questions\n",
    "rubric={points:10}\n",
    "\n",
    "Each question is worth 2 points. Max 2 sentences per answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem with calling `fit_transform` on your test data with `CountVectorizer`? \n",
    "2. Why is it important to follow the Golden Rule? If you violate it, will that give you a worse classifier?\n",
    "3. If you could only access one of `predict` or `predict_proba`, which one would you choose? Briefly explain.\n",
    "4. What are two advantages of using sklearn `Pipeline`s? \n",
    "5. What are two advantages of `RandomizedSearchCV` over `GridSearchCV`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1. We might have a different # of columns (unless we set a max_features) / different vocabulary features between train and test data - thus jumbling up what each feature column actually represents and messing up the model. Python might return an error if there is a mismatch in the # of columns between the train/test data, or even worse will not display an error if the number of columns are the same, but the features are different.\n",
    "2. The Golden Rule says that a test set should only be used 'once' (Lecture 4) so as to not influence the training data in any way because it must be as close to a proxy of deployment data as possible (unseen).  Violation will not necessarily give a worse classifier (on test data), but might instead lead to a misleadingly optimistic test score that diverges further from the deployment score because the model was indirectly trained/affected by test data (a worse classifier on deployment).\n",
    "3. Assuming we have access to to the `classes_` field of the linear regression to interpret `predict_proba()`, `predict_proba()` would be more useful. While `predict` only returns the class with the higher probability, `predict_proba` provides the probabilities associated with each class, meaning it is easy to obtain the results of `predict`, but we get a more nuanced picture of which examples our model is more/less confident in predicting.\n",
    "4. Pipelines help avoid mistakes like violating the Golden Rule, refitting on the test split, while automatically making sure the same transformations take place on the train/test set. Moreover, they simplify and reduce human coding error in a model that may require many preprocessing steps and transformations, by providing a simple object to use for cross validation, fitting using `X_train`, and scoring using `X_test`.\n",
    "5. `RandomizedSearchCV` is advantageous because you can provide less restriction on which values to try, while choosing the number of runs. Additionally, it tends to be better in finding optimal values where variation in some hypterparameters are less important than others (more parameters do not affect effeciency) (Lecture 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Canvas\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "\n",
    "1. **NEW STEP**: if you run `pip install canvasutils -U` in your terminal with your environment activated, you should update to a newer version of `canvasutils`. This new version should address a couple pain points:\n",
    "  - The dropdown menu should now be sorted from most to least recent, meaning the default will be hw3.\n",
    "  - For those having trouble with the Jupyter widgets and the dropdowns: if you add the argument `no_widgets=True` to your `submit` call, it should let you do a text-based entry of your key and avoid the dropdowns altogether.\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`.\n",
    "2. Save your notebook.\n",
    "3. Convert your notebook to `.html` format using the `convert_notebook()` function below **or** by `File -> Export Notebook As... -> Export Notebook to HTML`.\n",
    "4. Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    ">For this step, you will need a Canvas *Access Token* token. If you haven't already got one, log-in to Canvas, click `Account` (top-left of the screen), then `Settings`, then scroll down until you see the `+ New Access Token` button. Click that button, give your token any name you like and set the expiry date to Dec 31, 2020. Then click `Generate token`. Save this token in a safe place on your computer as you'll need it for all assignments. Treat the token with as much care as you would an important password. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasutils.submit import submit, convert_notebook\n",
    "\n",
    "# Note: the canvasutils package should have been installed as part of your environment setup - \n",
    "# see https://github.com/UBC-CS/cpsc330/blob/master/docs/setup.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_notebook(\"hw3.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook to HTML (or you can convert manually from the File menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit(course_code=53561, token=False)  # uncomment and run when ready to submit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
